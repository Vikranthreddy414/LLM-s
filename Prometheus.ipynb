{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llama_dataset import download_llama_dataset\n",
        "\n",
        "llama2_rag_dataset, llama2_documents = download_llama_dataset(\n",
        "    \"Llama2PaperDataset\", \"./data/llama2\"\n",
        ")"
      ],
      "metadata": {
        "id": "_AbmmF1Aq1eN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import HuggingFaceInferenceAPI\n",
        "import os\n",
        "\n",
        "HF_TOKEN = \"hf_##################\"\n",
        "HF_ENDPOINT_URL = \"#############\"\n",
        "\n",
        "prometheus_llm = HuggingFaceInferenceAPI(\n",
        "    model_name=HF_ENDPOINT_URL,\n",
        "    token=HF_TOKEN,\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\n",
        "\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "gpt4_llm = OpenAI(\"gpt-4\")"
      ],
      "metadata": {
        "id": "Ov4ia9oIq8Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FhZTYzOnrvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correctness Evaluation Prompt:"
      ],
      "metadata": {
        "id": "he-SV3ECrwDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prometheus_correctness_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "   1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "   2. After writing a feedback, write a score that is either 1 or 2 or 3 or 4 or 5. You should refer to the score rubric.\n",
        "   3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] (1 or 2 or 3 or 4 or 5)'\n",
        "   4. Please do not generate any other opening, closing, and explanations.\n",
        "   5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.\n",
        "\n",
        "   ###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}\n",
        "\n",
        "   ###Generate answer to evaluate: {generated_answer}\n",
        "\n",
        "   ###Reference Answer (Score 5): {reference_answer}\n",
        "\n",
        "   ###Score Rubrics:\n",
        "   Score 1: If the generated answer is not relevant to the user query and reference answer.\n",
        "   Score 2: If the generated answer is according to reference answer but not relevant to user query.\n",
        "   Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n",
        "   Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n",
        "   Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n",
        "\n",
        "   ###Feedback:\"\"\"\n",
        "Faithfulness Evaluation Prompt:\n",
        "\n",
        "prometheus_faithfulness_eval_prompt_template= \"\"\"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.\n",
        "1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n",
        "2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
        "3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
        "4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)”\n",
        "5. Please do not generate any other opening, closing, and explanations.\n",
        "\n",
        "###The instruction to evaluate: Your task is to evaluate if the given piece of information is supported by context.\n",
        "\n",
        "###Information: {query_str}\n",
        "\n",
        "###Context: {context_str}\n",
        "\n",
        "###Score Rubrics:\n",
        "Score YES: If the given piece of information is supported by context.\n",
        "Score NO: If the given piece of information is not supported by context\n",
        "\n",
        "###Feedback: \"\"\"\n",
        "\n",
        "prometheus_faithfulness_refine_prompt_template= \"\"\"###Task Description: An instruction (might include an Input inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.\n",
        "1. You are provided with evaluation task with the help of information, context information and an existing answer.\n",
        "2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
        "3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
        "4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\n",
        "5. Please do not generate any other opening, closing, and explanations.\n",
        "\n",
        "###The instruction to evaluate: If the information is present in the context and also provided with an existing answer.\n",
        "\n",
        "###Existing answer: {existing_answer}\n",
        "\n",
        "###Information: {query_str}\n",
        "\n",
        "###Context: {context_msg}\n",
        "\n",
        "###Score Rubrics:\n",
        "Score YES: If the existing answer is already YES or If the Information is present in the context.\n",
        "Score NO: If the existing answer is NO and If the Information is not present in the context.\n",
        "\n",
        "###Feedback: \"\"\""
      ],
      "metadata": {
        "id": "uUukvplEref_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faithfulness Evaluation Prompt:"
      ],
      "metadata": {
        "id": "tv-hCPskr31q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prometheus_faithfulness_eval_prompt_template= \"\"\"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.\n",
        "1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n",
        "2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
        "3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
        "4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)”\n",
        "5. Please do not generate any other opening, closing, and explanations.\n",
        "\n",
        "###The instruction to evaluate: Your task is to evaluate if the given piece of information is supported by context.\n",
        "\n",
        "###Information: {query_str}\n",
        "\n",
        "###Context: {context_str}\n",
        "\n",
        "###Score Rubrics:\n",
        "Score YES: If the given piece of information is supported by context.\n",
        "Score NO: If the given piece of information is not supported by context\n",
        "\n",
        "###Feedback: \"\"\"\n",
        "\n",
        "prometheus_faithfulness_refine_prompt_template= \"\"\"###Task Description: An instruction (might include an Input inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.\n",
        "1. You are provided with evaluation task with the help of information, context information and an existing answer.\n",
        "2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
        "3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
        "4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\n",
        "5. Please do not generate any other opening, closing, and explanations.\n",
        "\n",
        "###The instruction to evaluate: If the information is present in the context and also provided with an existing answer.\n",
        "\n",
        "###Existing answer: {existing_answer}\n",
        "\n",
        "###Information: {query_str}\n",
        "\n",
        "###Context: {context_msg}\n",
        "\n",
        "###Score Rubrics:\n",
        "Score YES: If the existing answer is already YES or If the Information is present in the context.\n",
        "Score NO: If the existing answer is NO and If the Information is not present in the context.\n",
        "\n",
        "###Feedback: \"\"\""
      ],
      "metadata": {
        "id": "3MS3RHBLrfCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevancy Evaluation Prompt:"
      ],
      "metadata": {
        "id": "CjjM5Zvmr8wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prometheus_relevancy_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given.\n",
        "       1. You are provided with evaluation task with the help of a query with response and context.\n",
        "       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
        "       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
        "       4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)”\n",
        "       5. Please do not generate any other opening, closing, and explanations.\n",
        "\n",
        "        ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n",
        "\n",
        "        ###Query and Response: {query_str}\n",
        "\n",
        "        ###Context: {context_str}\n",
        "\n",
        "        ###Score Rubrics:\n",
        "        Score YES: If the response for the query is in line with the context information provided.\n",
        "        Score NO: If the response for the query is not in line with the context information provided.\n",
        "\n",
        "        ###Feedback: \"\"\"\n",
        "\n",
        "prometheus_relevancy_refine_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, an existing answer, and a score rubric representing a evaluation criteria are given.\n",
        "   1. You are provided with evaluation task with the help of a query with response and context and an existing answer.\n",
        "   2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n",
        "   3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n",
        "   4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\n",
        "   5. Please do not generate any other opening, closing, and explanations.\n",
        "\n",
        "   ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n",
        "\n",
        "   ###Query and Response: {query_str}\n",
        "\n",
        "   ###Context: {context_str}\n",
        "\n",
        "   ###Score Rubrics:\n",
        "   Score YES: If the existing answer is already YES or If the response for the query is in line with the context information provided.\n",
        "   Score NO: If the existing answer is NO and If the response for the query is in line with the context information provided.\n",
        "\n",
        "   ###Feedback: \"\"\""
      ],
      "metadata": {
        "id": "SdTpIVGzr67E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluators"
      ],
      "metadata": {
        "id": "VzqKLgQNsCNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import ServiceContext\n",
        "from llama_index.evaluation import (\n",
        "    CorrectnessEvaluator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator,\n",
        ")\n",
        "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
        "import tiktoken\n",
        "\n",
        "# Provide Prometheus model in service_context\n",
        "prometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)\n",
        "\n",
        "# CorrectnessEvaluator with Prometheus model\n",
        "prometheus_correctness_evaluator = CorrectnessEvaluator(\n",
        "    service_context=prometheus_service_context,\n",
        "    parser_function=parser_function,\n",
        "    eval_template=prometheus_correctness_eval_prompt_template,\n",
        ")\n",
        "\n",
        "# FaithfulnessEvaluator with Prometheus model\n",
        "prometheus_faithfulness_evaluator = FaithfulnessEvaluator(\n",
        "    service_context=prometheus_service_context,\n",
        "    eval_template=prometheus_faithfulness_eval_prompt_template,\n",
        "    refine_template=prometheus_faithfulness_refine_prompt_template,\n",
        ")\n",
        "\n",
        "# RelevancyEvaluator with Prometheus model\n",
        "prometheus_relevancy_evaluator = RelevancyEvaluator(\n",
        "    service_context=prometheus_service_context,\n",
        "    eval_template=prometheus_relevancy_eval_prompt_template,\n",
        "    refine_template=prometheus_relevancy_refine_prompt_template,\n",
        ")\n",
        "\n",
        "# Set the encoding model to `gpt-4` for token counting.\n",
        "token_counter = TokenCountingHandler(\n",
        "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
        ")\n",
        "\n",
        "callback_manager = CallbackManager([token_counter])\n",
        "\n",
        "# Provide GPT-4 model in service_context\n",
        "gpt4_service_context = ServiceContext.from_defaults(\n",
        "    llm=gpt4_llm, callback_manager=callback_manager\n",
        ")\n",
        "\n",
        "# CorrectnessEvaluator with GPT-4 model\n",
        "gpt4_correctness_evaluator = CorrectnessEvaluator(\n",
        "    service_context=gpt4_service_context,\n",
        "    # parser_function=parser_function,\n",
        ")\n",
        "\n",
        "# FaithfulnessEvaluator with GPT-4 model\n",
        "gpt4_faithfulness_evaluator = FaithfulnessEvaluator(\n",
        "    service_context=gpt4_service_context,\n",
        "    eval_template=prometheus_faithfulness_eval_prompt_template,\n",
        "    refine_template=prometheus_faithfulness_refine_prompt_template,\n",
        ")\n",
        "\n",
        "# RelevancyEvaluator with GPT-4 model\n",
        "gpt4_relevancy_evaluator = RelevancyEvaluator(\n",
        "    service_context=gpt4_service_context,\n",
        "    eval_template=prometheus_relevancy_eval_prompt_template,\n",
        "    refine_template=prometheus_relevancy_refine_prompt_template,\n",
        ")\n",
        "\n",
        "# create a dictionary of evaluators\n",
        "prometheus_evaluators = {\n",
        "    \"correctness\": prometheus_correctness_evaluator,\n",
        "    \"faithfulness\": prometheus_faithfulness_evaluator,\n",
        "    \"relevancy\": prometheus_relevancy_evaluator,\n",
        "}\n",
        "\n",
        "gpt4_evaluators = {\n",
        "    \"correctness\": gpt4_correctness_evaluator,\n",
        "    \"faithfulness\": gpt4_faithfulness_evaluator,\n",
        "    \"relevancy\": gpt4_relevancy_evaluator,\n",
        "}"
      ],
      "metadata": {
        "id": "sgJEPywvr_6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ujf1In5BsFMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Run batch evaluations on defined evaluators"
      ],
      "metadata": {
        "id": "yNJNvKTwsLwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation import BatchEvalRunner\n",
        "\n",
        "\n",
        "async def batch_eval_runner(\n",
        "    evaluators, query_engine, questions, reference=None, num_workers=8\n",
        "):\n",
        "    batch_runner = BatchEvalRunner(\n",
        "        evaluators, workers=num_workers, show_progress=True\n",
        "    )\n",
        "\n",
        "    eval_results = await batch_runner.aevaluate_queries(\n",
        "        query_engine, queries=questions, reference=reference\n",
        "    )\n",
        "\n",
        "    return eval_results"
      ],
      "metadata": {
        "id": "Yox124idsHh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ls74c0TBsNZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Query Engine, Questions, and References."
      ],
      "metadata": {
        "id": "531odujDsSte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine, rag_dataset = create_query_engine_rag_dataset(\"./data/llama2\")\n",
        "\n",
        "questions = [example.query for example in rag_dataset.examples]\n",
        "\n",
        "reference = [[example.reference_answer] for example in rag_dataset.examples]"
      ],
      "metadata": {
        "id": "kZ38I-G7sQ5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute Correctness, Faithfulness, and Relevancy Evaluation."
      ],
      "metadata": {
        "id": "T2ChCn3-sVQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prometheus_eval_results = await batch_eval_runner(\n",
        "    prometheus_evaluators, query_engine, questions, reference\n",
        ")\n",
        "\n",
        "gpt4_eval_results = await batch_eval_runner(\n",
        "    gpt4_evaluators, query_engine, questions, reference\n",
        ")"
      ],
      "metadata": {
        "id": "oIfdZInrsTbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}